{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "最简单的 LlamaIndex 示例，读取 PDF 文档并构建索引。不需要自己构建向量数据库。\n",
    "使用SimpleDirectoryReader读取文件夹中的所有 pdf 文件时，碰到问题，读取到的都是乱码，后续用 pdfreader 读取"
   ],
   "id": "226a0240297dc19b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-25T07:02:03.156600Z",
     "start_time": "2025-09-25T07:02:03.152900Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from llama_cloud_services import LlamaParse\n",
    "from llama_index.core import Settings, SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.embeddings.dashscope import DashScopeTextEmbeddingModels\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "# 全局设置 llm\n",
    "Settings.llm = OpenAILike(model='qwen-max', api_base='https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "                          api_key=os.getenv(\"DASHSCOPE_API_KEY\"), is_chat_model=True)\n",
    "# 全局设置 embedding\n",
    "Settings.embed_model = DashScopeEmbeddings(\n",
    "                                           model=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1)\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T07:28:32.196419Z",
     "start_time": "2025-09-25T07:28:27.953947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.readers.file import PDFReader\n",
    "\n",
    "# 使用PDFReader读取PDF文档\n",
    "# pdf_reader = PDFReader()\n",
    "# documents = pdf_reader.load_data(\"./data/deepseek-v3-1-4.pdf\")\n",
    "documents = SimpleDirectoryReader(input_dir=\"./data\").load_data()\n",
    "print(documents[0].text[:500])\n",
    "# 构建索引\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"deepseek v3 有多少参数？\")\n",
    "response"
   ],
   "id": "29e926ea62f37f44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3 Technical Report\n",
      "DeepSeek-AI\n",
      "research@deepseek.com\n",
      "Abstract\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a mul\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 15:28:32,187 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='DeepSeek-V3 拥有总计671亿个参数，每个令牌激活37亿个参数。', source_nodes=[NodeWithScore(node=TextNode(id_='aeb80c7c-5f41-44ab-bcda-a1def5c149d0', embedding=None, metadata={'page_label': '1', 'file_name': 'deepseek-v3-1-4.pdf', 'file_path': '/Users/onepiecekevin/Documents/learn/ai/agent/rag_learn/第四章 LlamaIndex/data/deepseek-v3-1-4.pdf', 'file_type': 'application/pdf', 'file_size': 192218, 'creation_date': '2025-09-25', 'last_modified_date': '2025-03-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='77855cf2-2b3b-475f-88b6-02d7f68d4c34', node_type='4', metadata={'page_label': '1', 'file_name': 'deepseek-v3-1-4.pdf', 'file_path': '/Users/onepiecekevin/Documents/learn/ai/agent/rag_learn/第四章 LlamaIndex/data/deepseek-v3-1-4.pdf', 'file_type': 'application/pdf', 'file_size': 192218, 'creation_date': '2025-09-25', 'last_modified_date': '2025-03-12'}, hash='0515196efc62100e53b42908557442c2eed27cbc45875fe922ea68b3c26d90a3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025', mimetype='text/plain', start_char_idx=0, end_char_idx=1815, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.601293706528227), NodeWithScore(node=TextNode(id_='d4ff2dc4-5c13-4238-a585-d53ee88d1901', embedding=None, metadata={'page_label': '4', 'file_name': 'deepseek-v3-1-4.pdf', 'file_path': '/Users/onepiecekevin/Documents/learn/ai/agent/rag_learn/第四章 LlamaIndex/data/deepseek-v3-1-4.pdf', 'file_type': 'application/pdf', 'file_size': 192218, 'creation_date': '2025-09-25', 'last_modified_date': '2025-03-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='7dcd2d1e-57fe-46fa-8a24-de8660f3a2c9', node_type='4', metadata={'page_label': '4', 'file_name': 'deepseek-v3-1-4.pdf', 'file_path': '/Users/onepiecekevin/Documents/learn/ai/agent/rag_learn/第四章 LlamaIndex/data/deepseek-v3-1-4.pdf', 'file_type': 'application/pdf', 'file_size': 192218, 'creation_date': '2025-09-25', 'last_modified_date': '2025-03-12'}, hash='b5171bef03c2bc08032368e6cf986d0c09ef0fbd8fd3309976da97799b1983bb')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its\\nreasoning performance. Meanwhile, we also maintain control over the output style and\\nlength of DeepSeek-V3.\\nSummary of Core Evaluation Results\\n• Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA,\\nDeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9\\non MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source\\nmodels like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source\\nand closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3\\ndemonstrates superior performance among open-source models on both SimpleQA and\\nChinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual\\nknowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese\\nSimpleQA), highlighting its strength in Chinese factual knowledge.\\n• Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\\nmath-related benchmarks among all non-long-CoT open-source and closed-source models.\\nNotably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\\ndemonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\\nDeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\\nsuch as LiveCodeBench, solidifying its position as the leading model in this domain. For\\nengineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\\nit still outpaces all other models by a significant margin, demonstrating its competitiveness\\nacross diverse technical benchmarks.\\nIn the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\\nmodel architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\\nour compute clusters, the training framework, the support for FP8 training, the inference\\ndeployment strategy, and our suggestions on future hardware design. Next, we describe our\\npre-training process, including the construction of training data, hyper-parameter settings, long-\\ncontext extension techniques, the associated evaluations, as well as some discussions (Section 4).\\nThereafter, we discuss our efforts on post-training, which include Supervised Fine-Tuning (SFT),\\nReinforcement Learning (RL), the corresponding evaluations, and discussions (Section 5). Lastly,\\nwe conclude this work, discuss existing limitations of DeepSeek-V3, and propose potential\\ndirections for future research (Section 6).\\n2. Architecture\\nWe first introduce the basic architecture of DeepSeek-V3, featured by Multi-head Latent Atten-\\ntion (MLA) (DeepSeek-AI, 2024c) for efficient inference and DeepSeekMoE (Dai et al., 2024)\\nfor economical training. Then, we present a Multi-Token Prediction (MTP) training objective,\\nwhich we have observed to enhance the overall performance on evaluation benchmarks. For\\nother minor details not explicitly mentioned, DeepSeek-V3 adheres to the settings of DeepSeek-\\nV2 (DeepSeek-AI, 2024c).\\n2.1. Basic Architecture\\nThe basic architecture of DeepSeek-V3 is still within the Transformer (Vaswani et al., 2017)\\nframework. For efficient inference and economical training, DeepSeek-V3 also adopts MLA\\nand DeepSeekMoE, which have been thoroughly validated by DeepSeek-V2. Compared with\\nDeepSeek-V2, an exception is that we additionally introduce an auxiliary-loss-free load balancing\\n6', mimetype='text/plain', start_char_idx=0, end_char_idx=3475, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.5936150027611866)], metadata={'aeb80c7c-5f41-44ab-bcda-a1def5c149d0': {'page_label': '1', 'file_name': 'deepseek-v3-1-4.pdf', 'file_path': '/Users/onepiecekevin/Documents/learn/ai/agent/rag_learn/第四章 LlamaIndex/data/deepseek-v3-1-4.pdf', 'file_type': 'application/pdf', 'file_size': 192218, 'creation_date': '2025-09-25', 'last_modified_date': '2025-03-12'}, 'd4ff2dc4-5c13-4238-a585-d53ee88d1901': {'page_label': '4', 'file_name': 'deepseek-v3-1-4.pdf', 'file_path': '/Users/onepiecekevin/Documents/learn/ai/agent/rag_learn/第四章 LlamaIndex/data/deepseek-v3-1-4.pdf', 'file_type': 'application/pdf', 'file_size': 192218, 'creation_date': '2025-09-25', 'last_modified_date': '2025-03-12'}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T07:26:54.473921Z",
     "start_time": "2025-09-25T07:26:54.405607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reader = SimpleDirectoryReader(input_dir=\"./data\", recursive=False, required_exts=[\".pdf\"])\n",
    "documents = reader.load_data()\n",
    "# 第一页内容\n",
    "documents[0].text\n"
   ],
   "id": "41a56d524d633603",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DeepSeek-V3 Technical Report\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\\nparameters with 37B activated for each token. To achieve efficient inference and cost-effective\\ntraining, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\\ntures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\\nan auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\\nhigh-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\\nfully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\\nother open-source models and achieves performance comparable to leading closed-source\\nmodels. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\\nfor its full training. In addition, its training process is remarkably stable. Throughout the entire\\ntraining process, we did not experience any irrecoverable loss spikes or perform any rollbacks.\\nThe model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\\nMMLU-Pro\\n(EM)\\nGPQA-Diamond\\n(Pass@1)\\nMATH 500\\n(EM)\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n75.9\\n59.1\\n90.2\\n39.2\\n51.6\\n42.0\\n66.2\\n41.3\\n74.7\\n16.7\\n35.6\\n22.6\\n71.6\\n49.0\\n80.0\\n23.3 24.8 23.8\\n73.3\\n51.1\\n73.8\\n23.3\\n25.3 24.5\\n72.6\\n49.9\\n74.6\\n9.3\\n23.6\\n38.8\\n78.0\\n65.0\\n78.3\\n16.0\\n20.3\\n50.8\\nDeepSeek-V3 DeepSeek-V2.5 Qwen2.5-72B-Inst Llama-3.1-405B-Inst GPT-4o-0513 Claude-3.5-Sonnet-1022\\nFigure 1 |Benchmark performance of DeepSeek-V3 and its counterparts.\\narXiv:2412.19437v2  [cs.CL]  18 Feb 2025'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T08:01:48.495800Z",
     "start_time": "2025-09-25T08:01:39.197078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 用 llamaParse 解析，可以解析表格和图片\n",
    "from llama_cloud_services import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import os\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-IzCgUhy1kpqAKDffoD2UC0xYn3UlxjWcyTu5bvY1v2VwAhFU\"\n",
    "# markdown and text 都是可用的\n",
    "parser = LlamaParse(result_type=\"markdown\")\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"./data\", file_extractor=file_extractor, required_exts=[\".pdf\"]).load_data()\n",
    "print(documents[0].text)\n"
   ],
   "id": "df6508c3bdf6c4cf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 16:01:39,447 - INFO - NumExpr defaulting to 8 threads.\n",
      "2025-09-25 16:01:43,411 - INFO - HTTP Request: POST https://api.cloud.llamaindex.ai/api/parsing/upload \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id c5f1a02d-555a-4c8e-aa9b-de5ce07b311a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 16:01:44,947 - INFO - HTTP Request: GET https://api.cloud.llamaindex.ai/api/parsing/job/c5f1a02d-555a-4c8e-aa9b-de5ce07b311a \"HTTP/1.1 200 OK\"\n",
      "2025-09-25 16:01:47,609 - INFO - HTTP Request: GET https://api.cloud.llamaindex.ai/api/parsing/job/c5f1a02d-555a-4c8e-aa9b-de5ce07b311a \"HTTP/1.1 200 OK\"\n",
      "2025-09-25 16:01:48,328 - INFO - HTTP Request: GET https://api.cloud.llamaindex.ai/api/parsing/job/c5f1a02d-555a-4c8e-aa9b-de5ce07b311a/result/markdown \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arXiv:2412.19437v2 [cs.CL] 18 Feb 2025\n",
      "\n",
      "# DeepSeek-V3 Technical Report\n",
      "\n",
      "# DeepSeek-AI\n",
      "\n",
      "research@deepseek.com\n",
      "\n",
      "# Abstract\n",
      "\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.\n",
      "\n",
      "|          | DeepSeek-V3 | DeepSeek-V2.5 | Qwen2.5-72B-Inst | Llama-3.1-405B-Inst | GPT-4o-0513 | Claude-3.5-Sonnet-1022 |     |\n",
      "| -------- | ----------- | ------------- | ---------------- | ------------------- | ----------- | ---------------------- | --- |\n",
      "| 100      | 90.2        |               |                  |                     |             |                        |     |\n",
      "| 80       | 75.9        | 71.6          | 73.3             | 72.6                | 78.0        |                        |     |\n",
      "| (%)      | 66.2        | 65.0          | 59.1             |                     |             |                        |     |\n",
      "| 60       |             | 49.0          | 51.1             | 49.9                | 51.6        | 50.8                   |     |\n",
      "| Accuracy |             | 41.3          | 39.2             |                     | ~~38.8~~    | 35.6                   |     |\n",
      "| 20       |             |               | 16.7             | 16.0                | 20.3        |                        | 9.3 |\n",
      "| 0        | MMLU-Pro    | GPQA-Diamond  | MATH 500         | AIME 2024           | Codeforces  | SWE-bench Verified     |     |\n",
      "\n",
      "Figure 1 | Benchmark performance of DeepSeek-V3 and its counterparts.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:15:14.809710Z",
     "start_time": "2025-09-25T09:15:13.592013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.readers.web import SimpleWebPageReader\n",
    "\n",
    "# 读取网页\n",
    "documents = SimpleWebPageReader(html_to_text=True).load_data(urls=[\"https://edu.guangjuke.com/tx/\"])\n",
    "print(documents[0].text)"
   ],
   "id": "ecf7e6dc6f2a49b4",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/agent/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login\n",
      "\n",
      "__\n",
      "\n",
      "用户登录\n",
      "\n",
      "![](/api/Ajax/vertify/type/users_login.html)\n",
      "\n",
      "[ 登录](javascript:void\\(0\\))\n",
      "\n",
      "[忘记密码?](/user/Users/retrieve_password.html) [立即注册](/reg)\n",
      "\n",
      "快捷登录 | [__](/index.php?m=plugins&c=QqLogin&a=login)[__](/index.php?m=plugins&c=WxLogin&a=login)[__](/index.php?m=plugins&c=Wblogin&a=login)\n",
      "\n",
      "[\n",
      "![聚客AI学院大模型应用开发微调项目实践课程学习平台](https://oss.guangjuke.com/uploads/allimg/20250224/1-250224200331L9.png)\n",
      "](https://edu.guangjuke.com)\n",
      "\n",
      "[首页](https://edu.guangjuke.com)\n",
      "[全部课程](https://edu.guangjuke.com/shipinkecheng/)\n",
      "[大模型应用](https://edu.guangjuke.com/tx/)\n",
      "[精选好文](https://edu.guangjuke.com/haowen/)\n",
      "[聚客社区](https://edu.guangjuke.com/ask.html)\n",
      "[学员喜报](https://edu.guangjuke.com/xibao/)\n",
      "[关于我们](https://www.guangjuke.com/about/)\n",
      "\n",
      "[登录/注册](https://edu.guangjuke.com/user)\n",
      "\n",
      "![](/template/pc/static/images/banner-tip-bar.28e923ae.png)\n",
      "\n",
      "# 锤炼前沿实战精华，独创多领域大模型人才培养方案\n",
      "\n",
      "Kevin聚客科技联合创始人/技术总监（CTO）\n",
      "\n",
      "华为高级架构师互联网AI领域专家\n",
      "\n",
      "互联网后端技术领域15年从业经验，曾任职华为、新一代技术研究院，对Open AI、Azure AI、Google AI等大模型有丰富的实战项目经验。\n",
      "\n",
      "Aron人工智能研究院研究员\n",
      "\n",
      "人工智能算法研究员医疗领域AI专家\n",
      "\n",
      "8年深度学习算法研发经验，精通深度学习框架，有丰富的GPU模型加速，移动端模型加速，模型优化，模型部署经验 。\n",
      "\n",
      "BoboAI系列课程的布道者\n",
      "\n",
      "互联网AI技术专家企业高级架构师\n",
      "\n",
      "互联网后端技术领域15年从业经验，曾在知名企业用友、华电和百丽担任要职。历任高级软件开发工程师、系统架构师及首席技术官（CTO）。\n",
      "\n",
      "Anny国家工信部AI认证专家\n",
      "\n",
      "高级Python工程师首批大模型研发者\n",
      "\n",
      "数十年开发经验，深耕大数据、智能体、大模型垂直应用解决方案、AI应用工程等领域。\n",
      "\n",
      "Ray深度人工智能教育创始人\n",
      "\n",
      "高级AI算法工程师深度智谷科技创始人\n",
      "\n",
      "5年人工智能算法领域研发经验，6年人工智能教学经验，具备扎实的人工智能算法理论基础知识和丰富的项目实战经验.\n",
      "\n",
      "Cyber\n",
      "\n",
      "金融大厂架构师互联网连续创业者\n",
      "\n",
      "AI人工智能领域6年从业经验，对Open AI、Azure AI、Google AI、SD AI等AI大模型有丰富的实战项目经验。\n",
      "\n",
      "#### 福利一：高性能GPU资源\n",
      "\n",
      "提供学习阶段进行训练的线上实验室算力资源，帮助学生进行学习阶段的模型训练、练习。\n",
      "\n",
      "#### 福利二：大模型项目资源库\n",
      "\n",
      "超值附赠大量大模型项目资源库，包括丰富的代码示例和数据集，供学员在学习过程中使用和参考，加速学员的项目实践和技能提升。\n",
      "\n",
      "扫码添加专属老师  \n",
      "获取更多福利信息\n",
      "\n",
      "# 六大模块递进式学习，更顺滑、更高效实现大模型能力跃迁\n",
      "\n",
      "Hugging Face核心组件使用\n",
      "\n",
      "模型部署推理\n",
      "\n",
      "Datasets数据工程\n",
      "\n",
      "DeepSpeed分布式训练\n",
      "\n",
      "SFT微调训练\n",
      "\n",
      "模型合并、打包、部署\n",
      "\n",
      "模型量化核心算法与最佳实践\n",
      "\n",
      "模型蒸馏原理及动手实践\n",
      "\n",
      "模型评估方法和最佳实践\n",
      "\n",
      "项目  \n",
      "场景\n",
      "\n",
      "1、基于 Bert 的中文评价情感分析（分类任务）      2、定制化模型输出（生成任务）      3、基于特定数据集训练情绪对话模型\n",
      "\n",
      "RAG工程化\n",
      "\n",
      "Embedding Models嵌入模型原理\n",
      "\n",
      "Vector Store向量存储\n",
      "\n",
      "LlamaIndex框架深度应用\n",
      "\n",
      "Dify LLMOps\n",
      "\n",
      "RAG方案最佳实践\n",
      "\n",
      "项目  \n",
      "场景\n",
      "\n",
      "1、基于 DeepSeek + Dify 快速构建私有知识库      2、法律助手 - 基于 LlamaIndex + Chroma 构建法律条文助手\n",
      "\n",
      "智能体原理深度剖析\n",
      "\n",
      "强化学习\n",
      "\n",
      "Dify Agent应用\n",
      "\n",
      "LangGraph 框架深度学习\n",
      "\n",
      "项目  \n",
      "场景\n",
      "\n",
      "1、基于 Dify 快速构建智能体应用      2、基于 LangGraph 构建企业级复杂多代理应用\n",
      "\n",
      "深入理解DeepSeek设计思想和训练过程\n",
      "\n",
      "DeepSeek本地部署，多卡联合部署，vLLM多卡推理\n",
      "\n",
      "DeepSeek微调训练/多卡训练\n",
      "\n",
      "解锁  \n",
      "技能\n",
      "\n",
      "掌握 DeepSeek 本地部署及企业落地场景\n",
      "\n",
      "模态与多模态的概念\n",
      "\n",
      "多模态机器学习与典型任务\n",
      "\n",
      "本地私有化部署图文描述模型\n",
      "\n",
      "本地私有化部署文生视频模型\n",
      "\n",
      "本地部署 Llama-3.2-11B-Vision-Instruct-GGUF 实现视觉问答\n",
      "\n",
      "解锁  \n",
      "技能\n",
      "\n",
      "掌握多模态大模型解决典型任务：跨模态预训练/Language-Audio / Vision-Audio / Vision-Language/定位相关任务  \n",
      "Affect Computing 情感计算/Medical Image 医疗图像模态\n",
      "\n",
      "多套高薪Offer的AI大模型简历分享及参考\n",
      "\n",
      "简历项目1对1个性化指导\n",
      "\n",
      "技术模拟面试1对1指导\n",
      "\n",
      "解锁  \n",
      "技能\n",
      "\n",
      "高效写出高含金量的AI技术简历 / 项目场景模拟面试\n",
      "\n",
      "# 场景化深度落地实践，多重维度构建能力模型\n",
      "\n",
      "![](/template/pc/static/images/shizhan1.png)\n",
      "\n",
      "  * 基于LlamaIndex构建企业私有知识库（RAG项目） \n",
      "  * 基于Bert的中文评价情感分析（分类任务） \n",
      "  * 中文生成模型定制化（生成任务） \n",
      "  * 基于本地大模型的在线心理问诊系统（微调项目） \n",
      "  * 企业招标采购智能客服系统（RAG+微调项目） \n",
      "  * 基于YOLO的骨龄识别项目（视觉项目） \n",
      "  * 基于RAG的法律咨询智能助手 \n",
      "\n",
      "# 五大全方位闭环硬核服务，为你的学习和面试保驾护航\n",
      "\n",
      "#### 大家说好，才是真的好\n",
      "\n",
      "# 无论你是转行、进阶，都是你的不二之选\n",
      "\n",
      "AI大模型工程师专注于LLM领域的工程师，渴望深入探索大模型的高级应用和解决方案设计。\n",
      "\n",
      "NLP算法工程师寻求LLM实战经验，提升专业技能，增强大厂面试竞争力。\n",
      "\n",
      "AI算法工程师希望迅速掌握LLM技术，实现职业转型或深化AI领域技能。\n",
      "\n",
      "IT转行求职者对现有IT职业不满，寻求向AI领域转型的求职者，寻找技术转型的新起点，快速积累LLM实战经验。\n",
      "\n",
      "在职提升者在职工程师，希望提升LLM技能，提高工作效率，为职业晋升打下坚实基础。\n",
      "\n",
      "计算机应届毕业生计算机专业师生和应届毕业生，追求快速掌握LLM技能，增强就业竞争力。\n",
      "\n",
      "# 学前技术储备\n",
      "\n",
      "具备良好的编程能力，熟悉Python语言\n",
      "\n",
      "了解机器学习和深度学习的基本概念\n",
      "\n",
      "对大模型算法有初步的了解和兴趣\n",
      "\n",
      "帮助与常见问题\n",
      "\n",
      "Q：是否有基础要求？\n",
      "\n",
      "掌握基本的 Python 编程技能即可学习。\n",
      "\n",
      "Q：是否有详细的课程表？\n",
      "\n",
      "有的哦，可以联系官方客服领取。\n",
      "\n",
      "Q：上课形式和课时量是怎样的呢？\n",
      "\n",
      "课程为全程直播。课时2个月左右\n",
      "\n",
      "Q：直播是否有回看？\n",
      "\n",
      "直播的录播视频会上传到官网方便大家回看，但为了更好的学习互动效果，建议各位学员提前预留好时间，准备好问题，准时参加直播。\n",
      "\n",
      "Q: 课程录播视频的观看期限是多久？\n",
      "\n",
      "一年有效学习权益\n",
      "\n",
      "Q：可以跟老师互动交流吗？\n",
      "\n",
      "当然啦，我们会建立班级社群，群内可以互动交流。同时，大家还可以通过直播向老师提问。\n",
      "\n",
      "Q：报名缴费后可以退款吗？\n",
      "\n",
      "付款后 3 个自然日内，如果觉得课程不适合自己，可申请退款，超出 3 个自然日，就不再办理退款啦。退款流程预计为 10 个工作日。\n",
      "\n",
      "Q：可以分期付款吗？\n",
      "\n",
      "我们支持花呗信用卡分期付款。\n",
      "\n",
      "Q: 如何开发票，签合同？\n",
      "\n",
      "我们可以为学员开具正规的发票和合同。开发票相关事宜，请联系带班班主任。合同相关事宜，请联系报名老师。\n",
      "\n",
      "大厂标准培训\n",
      "\n",
      "海量精品课程\n",
      "\n",
      "汇聚优秀团队\n",
      "\n",
      "打造完善体系\n",
      "\n",
      "![](/template/pc/skin/images/cxun.gif) ![](/template/pc/skin/images/cxun2.gif)\n",
      "![](/template/pc/skin/images/cxun3.gif)\n",
      "\n",
      "Copyright © 2023-2025 聚客AI 版权所有  \n",
      "网站备案号：[湘ICP备2024094305号-1](https://beian.miit.gov.cn/)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. 文本切分与解析\n",
    "llamadeindex 将文本切分为多个节点，每个节点包含一个文本片段（chunk），以及该片段的元数据。"
   ],
   "id": "c5607edca92ded8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:40:50.536369Z",
     "start_time": "2025-09-25T09:40:50.485305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pydantic.v1 import BaseModel\n",
    "\n",
    "def show_json(data):\n",
    "    \"\"\"用于展示json数据\"\"\"\n",
    "    if isinstance(data, str):\n",
    "        obj = json.loads(data)\n",
    "        print(json.dumps(obj, indent=4, ensure_ascii=False))\n",
    "    elif isinstance(data, dict) or isinstance(data, list):\n",
    "        print(json.dumps(data, indent=4, ensure_ascii=False))\n",
    "    elif issubclass(type(data), BaseModel):\n",
    "        print(json.dumps(data.dict(), indent=4, ensure_ascii=False))\n",
    "\n",
    "def show_list_obj(data):\n",
    "    \"\"\"用于展示一组对象\"\"\"\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            show_json(item)\n",
    "    else:\n",
    "        raise ValueError(\"Input is not a list\")\n"
   ],
   "id": "8afc661d254a5148",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T09:42:21.590782Z",
     "start_time": "2025-09-25T09:42:21.574067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "\n",
    "node_parser = TokenTextSplitter(chunk_size=512, chunk_overlap=200)\n",
    "nodes = node_parser.get_nodes_from_documents(documents, show_progress=False)\n",
    "\n",
    "print(nodes[1].json())\n",
    "# print(nodes[2].json())\n"
   ],
   "id": "33397cd783dfb04d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id_\": \"6f914e0f-179d-4976-9dea-af09167f07ae\", \"embedding\": null, \"metadata\": {\"url\": \"https://edu.guangjuke.com/tx/\"}, \"excluded_embed_metadata_keys\": [], \"excluded_llm_metadata_keys\": [], \"relationships\": {\"1\": {\"node_id\": \"7c8de0e0-e53d-408b-95ed-bd161be8351b\", \"node_type\": \"4\", \"metadata\": {\"url\": \"https://edu.guangjuke.com/tx/\"}, \"hash\": \"491fa501355a604a7e0505d17f3bea07da28f97f0c192d8b7ad941c7a0ba1ea1\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"f0ff02e2-a035-4e5e-9d02-b71db211cdf1\", \"node_type\": \"1\", \"metadata\": {\"url\": \"https://edu.guangjuke.com/tx/\"}, \"hash\": \"717c00ec1955e32d1afeebf95ae577b94f9de33ecd0540b65486696aa0dbb4cf\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"a32d32b6-1ad0-43ba-a0bf-14aa322ded13\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"be74416c434cd27225f2bddd93b43b05843e97e1ee96f00b6760b045695178b0\", \"class_name\": \"RelatedNodeInfo\"}}, \"metadata_template\": \"{key}: {value}\", \"metadata_separator\": \"\\n\", \"text\": \"\\u9524\\u70bc\\u524d\\u6cbf\\u5b9e\\u6218\\u7cbe\\u534e\\uff0c\\u72ec\\u521b\\u591a\\u9886\\u57df\\u5927\\u6a21\\u578b\\u4eba\\u624d\\u57f9\\u517b\\u65b9\\u6848\\n\\nKevin\\u805a\\u5ba2\\u79d1\\u6280\\u8054\\u5408\\u521b\\u59cb\\u4eba/\\u6280\\u672f\\u603b\\u76d1\\uff08CTO\\uff09\\n\\n\\u534e\\u4e3a\\u9ad8\\u7ea7\\u67b6\\u6784\\u5e08\\u4e92\\u8054\\u7f51AI\\u9886\\u57df\\u4e13\\u5bb6\\n\\n\\u4e92\\u8054\\u7f51\\u540e\\u7aef\\u6280\\u672f\\u9886\\u57df15\\u5e74\\u4ece\\u4e1a\\u7ecf\\u9a8c\\uff0c\\u66fe\\u4efb\\u804c\\u534e\\u4e3a\\u3001\\u65b0\\u4e00\\u4ee3\\u6280\\u672f\\u7814\\u7a76\\u9662\\uff0c\\u5bf9Open AI\\u3001Azure AI\\u3001Google AI\\u7b49\\u5927\\u6a21\\u578b\\u6709\\u4e30\\u5bcc\\u7684\\u5b9e\\u6218\\u9879\\u76ee\\u7ecf\\u9a8c\\u3002\\n\\nAron\\u4eba\\u5de5\\u667a\\u80fd\\u7814\\u7a76\\u9662\\u7814\\u7a76\\u5458\\n\\n\\u4eba\\u5de5\\u667a\\u80fd\\u7b97\\u6cd5\\u7814\\u7a76\\u5458\\u533b\\u7597\\u9886\\u57dfAI\\u4e13\\u5bb6\\n\\n8\\u5e74\\u6df1\\u5ea6\\u5b66\\u4e60\\u7b97\\u6cd5\\u7814\\u53d1\\u7ecf\\u9a8c\\uff0c\\u7cbe\\u901a\\u6df1\\u5ea6\\u5b66\\u4e60\\u6846\\u67b6\\uff0c\\u6709\\u4e30\\u5bcc\\u7684GPU\\u6a21\\u578b\\u52a0\\u901f\\uff0c\\u79fb\\u52a8\\u7aef\\u6a21\\u578b\\u52a0\\u901f\\uff0c\\u6a21\\u578b\\u4f18\\u5316\\uff0c\\u6a21\\u578b\\u90e8\\u7f72\\u7ecf\\u9a8c\", \"mimetype\": \"text/plain\", \"start_char_idx\": 816, \"end_char_idx\": 1049, \"metadata_seperator\": \"\\n\", \"text_template\": \"{metadata_str}\\n\\n{content}\", \"class_name\": \"TextNode\"}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-25T10:26:42.243732Z",
     "start_time": "2025-09-25T10:26:35.415863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 向量检索\n",
    "import os\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import TokenTextSplitter, SentenceSplitter\n",
    "from llama_index.embeddings.dashscope import DashScopeTextEmbeddingModels\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "# 设置 embedding 模型\n",
    "embed_model = DashScopeEmbeddings(\n",
    "    model=DashScopeTextEmbeddingModels.TEXT_EMBEDDING_V1\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(input_dir=\"./data\", required_exts=[\".pdf\"]).load_data()\n",
    "node_parser = TokenTextSplitter(chunk_size=512, chunk_overlap=200)\n",
    "\n",
    "# 切分文档\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "# 构建 index，指定 embedding 模型\n",
    "index = VectorStoreIndex(nodes, embed_model=embed_model)\n",
    "\n",
    "# 持久化\n",
    "# index.storage_context.persist(persist_dir=\"./doc_emb\")\n",
    "# 获取 reriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "results = vector_retriever.retrieve(\"deepseek v3 数学能力怎么样\")\n",
    "print(results[0].text)\n",
    "\n",
    "llm = OpenAILike(model='qwen-max', api_base='https://dashscope.aliyuncs.com/compatible-mode/v1',\n",
    "                     api_key=os.getenv(\"DASHSCOPE_API_KEY\"), is_chat_model=True)\n",
    "qa_engine = index.as_query_engine(llm=llm)\n",
    "response = qa_engine.query(\"deepseek v3数学能力怎么样?\")\n",
    "\n",
    "print(response)"
   ],
   "id": "88515b2482a77717",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its\n",
      "reasoning performance. Meanwhile, we also maintain control over the output style and\n",
      "length of DeepSeek-V3.\n",
      "Summary of Core Evaluation Results\n",
      "• Knowledge: (1) On educational benchmarks such as MMLU, MMLU-Pro, and GPQA,\n",
      "DeepSeek-V3 outperforms all other open-source models, achieving 88.5 on MMLU, 75.9\n",
      "on MMLU-Pro, and 59.1 on GPQA. Its performance is comparable to leading closed-source\n",
      "models like GPT-4o and Claude-Sonnet-3.5, narrowing the gap between open-source\n",
      "and closed-source models in this domain. (2) For factuality benchmarks, DeepSeek-V3\n",
      "demonstrates superior performance among open-source models on both SimpleQA and\n",
      "Chinese SimpleQA. While it trails behind GPT-4o and Claude-Sonnet-3.5 in English factual\n",
      "knowledge (SimpleQA), it surpasses these models in Chinese factual knowledge (Chinese\n",
      "SimpleQA), highlighting its strength in Chinese factual knowledge.\n",
      "• Code, Math, and Reasoning: (1) DeepSeek-V3 achieves state-of-the-art performance on\n",
      "math-related benchmarks among all non-long-CoT open-source and closed-source models.\n",
      "Notably, it even outperforms o1-preview on specific benchmarks, such as MATH-500,\n",
      "demonstrating its robust mathematical reasoning capabilities. (2) On coding-related tasks,\n",
      "DeepSeek-V3 emerges as the top-performing model for coding competition benchmarks,\n",
      "such as LiveCodeBench, solidifying its position as the leading model in this domain. For\n",
      "engineering-related tasks, while DeepSeek-V3 performs slightly below Claude-Sonnet-3.5,\n",
      "it still outpaces all other models by a significant margin, demonstrating its competitiveness\n",
      "across diverse technical benchmarks.\n",
      "In the remainder of this paper, we first present a detailed exposition of our DeepSeek-V3\n",
      "model architecture (Section 2). Subsequently, we introduce our infrastructures, encompassing\n",
      "our compute clusters, the training framework, the support for FP8 training, the inference\n",
      "deployment strategy, and our suggestions on future hardware design.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 18:26:42,237 - INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-V3在数学相关基准测试中表现出色，特别是在非长链思维链（non-long-CoT）的开源和闭源模型中达到了最先进水平。它在特定的基准测试如MATH-500上甚至超过了o1-preview的表现，这表明它具有强大的数学推理能力。\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
