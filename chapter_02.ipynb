{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-30T09:57:17.412879Z",
     "start_time": "2025-08-30T09:57:16.919292Z"
    }
   },
   "source": [
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.chains.openai_functions.openapi import openapi_spec_to_openai_fn\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.callbacks.manager import get_openai_callback\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sympy import pprint\n",
    "\n",
    "\n",
    "# from setuptools.sandbox import save_argv\n",
    "\n",
    "\n",
    "def extract_text_with_page_numbers(pdf: PdfReader) -> Tuple[str, List[int]]:\n",
    "    \"\"\"\n",
    "    从 PDF 中提取文本并记录每行文本对应的页码\n",
    "    :param pdf:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    page_numbers = []\n",
    "\n",
    "    for page_number, page in enumerate(pdf.pages, start=1):\n",
    "        extracted_text = page.extract_text()\n",
    "        if extracted_text:\n",
    "            text += extracted_text\n",
    "            page_numbers.extend([page_number] * len(extracted_text.split(\"\\n\")))\n",
    "        else:\n",
    "            logging.warning(f\"No text found on page {page_number}\")\n",
    "    return text, page_numbers\n",
    "\n",
    "def process_text_with_splitter(text: str, page_numbers: List[int], save_path: str = None) -> FAISS:\n",
    "    \"\"\"\n",
    "    处理文本并创建向量存储\n",
    "    :param text: 提取的文本内容\n",
    "    :param page_numbers: 每行文本对应的页码列表\n",
    "    :param save_path: 可选，保存向量数据库的路径\n",
    "    :return: 基于 FAISS 的向量存储对象\n",
    "    \"\"\"\n",
    "    # 创建文本分割器，用于将长文本分割成小块\n",
    "    text_splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\", \"\\n\\n\", \".\", \" \", \"\"], chunk_size=512, chunk_overlap=128, length_function=len)\n",
    "\n",
    "    # 分割文本\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    print(f\"文本被分割成 {len(chunks)} 个块\")\n",
    "\n",
    "    # openai 的嵌入模型\n",
    "    # ai_embeddings = OpenAIEmbeddings\n",
    "    # 阿里的嵌入模型\n",
    "    embeddings = DashScopeEmbeddings(model=\"text-embedding-v2\")\n",
    "\n",
    "    # 从文本块创建知识库\n",
    "    knowledgeBase = FAISS.from_texts(chunks, embeddings)\n",
    "    print(\"已从文本块创建知识库\")\n",
    "\n",
    "    # 存储每个文件块对应的页码信息\n",
    "    page_info = {chunk: page_numbers[i] for i, chunk in enumerate(chunks)}\n",
    "    knowledgeBase.page_info = page_info\n",
    "\n",
    "    if save_path:\n",
    "        # 确保目录存在\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # 保存 FAISS 向量数据库\n",
    "        knowledgeBase.save_local(save_path)\n",
    "        print(f\"向量数据库已保存到： {save_path}\")\n",
    "\n",
    "        # 保存页码信息到同一目录\n",
    "        with open(os.path.join(save_path, \"page_info.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(page_info, f)\n",
    "        print(f\"页码信息已保存到：{os.path.join(save_path, 'page_info.pkl')}\")\n",
    "\n",
    "    return knowledgeBase\n",
    "\n",
    "def load_knowledge_base(load_path: str, embeddings = None) -> FAISS:\n",
    "    \"\"\"\n",
    "    从磁盘加载向量数据库和页码信息\n",
    "    :param load_path: 向量数据库的保存路径\n",
    "    :param embeddings: 可选，嵌入模型，如果为 None，则创建一个新的 DashScopeEmbedding 模型\n",
    "    :return: 加载的 FAISS 向量数据库对象\n",
    "    \"\"\"\n",
    "    # 如果没有提供嵌入模型，则创建一个新的\n",
    "    if embeddings is None:\n",
    "        embeddings = DashScopeEmbeddings(model=\"text-embedding-v2\")\n",
    "\n",
    "    # 加载 FAISS 向量数据库，添加 allow_dangerous_deserialization 参数允许反序列化\n",
    "    knowledge_base = FAISS.load_local(load_path, embeddings, allow_dangerous_deserialization=True)\n",
    "    print(f\"向量数据库已从{load_path}加载\")\n",
    "\n",
    "    # 加载页码信息\n",
    "    page_info_path = os.path.join(load_path, \"pae_info.pkl\")\n",
    "    if os.path.exists(page_info_path):\n",
    "        with open(page_info_path, \"rb\") as f:\n",
    "            page_info = pickle.load(f)\n",
    "        knowledge_base.page_info = page_info\n",
    "        print(\"页码信息已加载\")\n",
    "    else:\n",
    "        print(\"未找到页码信息文件\")\n",
    "    return knowledge_base\n",
    "\n",
    "pdf_reader = PdfReader(\"./浦发上海浦东发展银行西安分行个金客户经理考核办法.pdf\")\n",
    "# 提取文本和页码信息\n",
    "text, page_numbers = extract_text_with_page_numbers(pdf_reader)\n",
    "\n",
    "text\n",
    "page_numbers"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T10:00:52.109001Z",
     "start_time": "2025-08-30T10:00:49.688200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"DASHSCOPE_API_KEY\"] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "print(f\"提取的文本长度：{len(text)} 个字符\")\n",
    "\n",
    "# 处理文本并创建知识库，同时保存到磁盘\n",
    "save_dir = \"./vector_db\"\n",
    "knowledge_base = process_text_with_splitter(text, page_numbers, save_path=save_dir)\n",
    "\n",
    "# 处理文本并创建知识库\n",
    "knowledge_base = process_text_with_splitter(text, page_numbers)\n",
    "\n",
    "knowledge_base"
   ],
   "id": "4fdedd690c334be4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "提取的文本长度：3881 个字符\n",
      "文本被分割成 10 个块\n",
      "已从文本块创建知识库\n",
      "向量数据库已保存到： ./vector_db\n",
      "页码信息已保存到：./vector_db/page_info.pkl\n",
      "文本被分割成 10 个块\n",
      "已从文本块创建知识库\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x147319a30>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-30T10:01:23.903640Z",
     "start_time": "2025-08-30T10:01:22.174791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 设置查询问题\n",
    "query = \"客户经理被投诉了，投诉一次扣多少分\"\n",
    "# query = \"客户经理每年评聘申报时间是怎样的\"\n",
    "if query:\n",
    "    # 执行相似度搜索，找到与查询相关的文档\n",
    "    docs = knowledge_base.similarity_search(query)\n",
    "\n",
    "    # 初始化大模型\n",
    "    llm = ChatOpenAI(model=\"qwen-plus\", temperature=0, base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\", api_key=os.getenv(\"DASHSCOPE_API_KEY\"))\n",
    "\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "    # 准备输入数据\n",
    "    input_data = {\"input_documents\": docs, \"question\": query}\n",
    "\n",
    "    # 使用回调函数跟踪 API 调用成本\n",
    "    with get_openai_callback() as cb:\n",
    "        response = chain.invoke(input=input_data)\n",
    "        # pprint.pprint(response)\n",
    "        print(f\"查询已处理，成本：{cb}\")\n",
    "        print(response[\"output_text\"])\n",
    "        print(\"来源：\")\n",
    "\n",
    "    # 记录唯一的页码\n",
    "    unique_pages = set()\n",
    "\n",
    "    # 显示每个文档块的来源页码\n",
    "    for doc in docs:\n",
    "        text_content = getattr(doc, \"page_content\", \"\")\n",
    "        source_page = knowledge_base.page_info.get(text_content.strip(), \"未知\")\n",
    "\n",
    "        if source_page not in unique_pages:\n",
    "            unique_pages.add(source_page)\n",
    "            print(f\"文本块页码：{source_page}\")\n",
    "\n"
   ],
   "id": "fe176c11d4308f17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询已处理，成本：Tokens Used: 1357\n",
      "\tPrompt Tokens: 1337\n",
      "\tCompletion Tokens: 20\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n",
      "根据提供的信息，客户经理如果被客户投诉，每次投诉会扣 **2分**。\n",
      "来源：\n",
      "文本块页码：1\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
